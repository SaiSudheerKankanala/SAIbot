{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "text_sources = {\n",
        "    'ipc': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/ipc.txt',\n",
        "    'gita': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/Bhagavad-gita_As_It_Is.txt',\n",
        "    'quran': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/quran-allah.txt',\n",
        "    'bible': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/Bible.txt',\n",
        "    'constitution': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/indian%20constitution.txt',\n",
        "    'garuda': 'https://raw.githubusercontent.com/SaiSudheerKankanala/SAIbot/main/GarudaPurana.txt'\n",
        "}\n",
        "\n",
        "def load_text(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "text_data = {name: load_text(url) for name, url in text_sources.items()}\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\nSECTION\", \"\\n\\nVERSE\", \"\\n\\nARTICLE\", \"\\n\\n\"]\n",
        ")\n",
        "\n",
        "chunks = []\n",
        "for name, text in text_data.items():\n",
        "    for chunk in text_splitter.split_text(text):\n",
        "        chunks.append({\"text\": chunk, \"source\": name})\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in chunks]\n",
        "metadatas = [{\"source\": chunk[\"source\"]} for chunk in chunks]\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_db = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedder,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=\"cpu\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    return generator(input_text, max_length=200)[0]['generated_text']\n",
        "\n",
        "def answer_question(question, k=3):\n",
        "    docs = vector_db.similarity_search(question, k=k)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    sources = \", \".join(set(doc.metadata[\"source\"] for doc in docs))\n",
        "    answer = generate_answer(question, context)\n",
        "    return f\"Answer: {answer}\\nSources: {sources}\"\n",
        "question = input(\"\")\n",
        "print(answer_question(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPDg5fCCqmO_",
        "outputId": "bc44d457-6646-4e05-d10e-292268024d28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "punishment for rape \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: death, or rigorous imprisonment for a term which shall not be less than twenty years, but which may extend to imprisonment for life, which shall mean imprisonment for the remainder of that personâ€™s natural life, and shall also be liable to fine\n",
            "Sources: ipc\n"
          ]
        }
      ]
    }
  ]
}